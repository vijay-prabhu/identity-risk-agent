{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - MLflow Experiment Tracking\n",
    "\n",
    "This notebook demonstrates MLflow integration for experiment tracking.\n",
    "\n",
    "## Key Features\n",
    "- **Experiment Tracking**: Log parameters, metrics, and artifacts\n",
    "- **Model Registry**: Version and manage models\n",
    "- **Reproducibility**: Track all training runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from src.models.mlflow_tracking import (\n",
    "    setup_mlflow,\n",
    "    MLflowExperimentTracker,\n",
    "    log_training_run,\n",
    "    get_best_model,\n",
    ")\n",
    "from src.models.risk_model import FEATURE_COLUMNS, prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = setup_mlflow()\n",
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model with Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_parquet('../data/features.parquet')\n",
    "X_train, X_test, y_train, y_test, scaler = prepare_data(df)\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with MLflow tracking\n",
    "with MLflowExperimentTracker(\"rf_baseline\", tags={\"model_type\": \"random_forest\"}) as tracker:\n",
    "    # Log parameters\n",
    "    params = {\n",
    "        \"n_estimators\": 100,\n",
    "        \"max_depth\": 10,\n",
    "        \"class_weight\": \"balanced\",\n",
    "    }\n",
    "    tracker.log_params(params)\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(**params, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    # Log metrics\n",
    "    tracker.log_metrics({\"auc\": auc, \"accuracy\": model.score(X_test, y_test)})\n",
    "    \n",
    "    # Log model\n",
    "    tracker.log_model(model, registered_model_name=\"identity-risk-model\")\n",
    "    \n",
    "    print(f\"Run ID: {tracker.run_id}\")\n",
    "    print(f\"AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all runs\n",
    "runs = mlflow.search_runs(experiment_names=[\"identity-risk-scoring\"])\n",
    "print(f\"Total runs: {len(runs)}\")\n",
    "runs[['run_id', 'metrics.auc', 'params.n_estimators', 'params.max_depth']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    best = get_best_model(metric=\"auc\")\n",
    "    print(f\"Best Run ID: {best['run_id']}\")\n",
    "    print(f\"Best AUC: {best['metrics'].get('auc', 'N/A')}\")\n",
    "    print(f\"Model URI: {best['model_uri']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow UI\n",
    "\n",
    "To view the MLflow UI, run:\n",
    "```bash\n",
    "mlflow ui --port 5000\n",
    "```\n",
    "Then open http://localhost:5000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
